{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Project: Training a Siamese Neural Network (SNN) to Detect Duplicate Question Pairs\n",
    "**Michael Klein**  \n",
    "16 December 2022  \n",
    "CS 539 Machine Learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project utilizes natural language processing (NLP) techniques together with Siamese Neural Networks (SNNs) to classify pairs of questions as duplicates or not duplicates. Unlike conventional classification problems, the machine learning model must be designed to accept two inputs and compare them to each other rather than to the rest of the training set.\n",
    "\n",
    "The data was sourced from [Quora’s Question Pairs Dataset](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs), which provides 400,000 question pairs as well as class labels (`is_duplicate`, 0 or 1).\n",
    "\n",
    "Solving problems like this one can go a long way toward improving search engine results and chatbot interactions as well as facial recognition and similar technologies (when extended to accept image input rather than character strings).\n",
    "\n",
    "The model is extensively developed using Keras, and data are pre-processed with Keras and the nltk module in Python. Though the initial model results were disappointing (no better than random guessing), the revised model showed significant improvement in accuracy (~86% after 50 epochs, ~80% on the validation set). Though this can use some improvement, these results, which have been the culmination of a semester's worth of work, are a good starting point for fine-tuning the model for business use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This final project uses a Siamese neural network (SNN) model to assess whether pairs of lexically similar questions are semantically similar, that is, if two similar-worded questions are actually asking the same thing.\n",
    "\n",
    "These special types of neural networks accept two inputs and compare features to each to determine whether the two inputs are similar by comparing their features. Inputs in an SNN model are passed through two identical sub-networks that use the same parameters and weights (when a weight is changed in one subnetwork, the other subnetwork adjusts to match).\n",
    "\n",
    "SNNs are commonly used in facial recognition technology and are the reason why your smartphone only needs a few images of your face rather than thousands of images (as would be the case with a conventional CNN).  \n",
    "\n",
    "<img src=\"./images/snn_example.png\" style=\"width: 600px;\"/>  \n",
    "\n",
    "Example SNN used for validating signatures. *Source: [Towards Data Science](https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there was no specific lecture or article that galvanized me to undertake this project, I did begin this course with an general idea of what I wanted to focus on. After taking the course CS 548 (Knowledge Discover and Data Mining), I knew that I wanted to learn more about neural networks, in particular convolutional neural networks (CNNs) for time-series analysis. At the same time, a friend of mine was looking for ways to build on a chatbot used to help participants in remote project work in the genomics space. I thought learning SNNs was a good overlap of my interests and practical real-world applications.  \n",
    "\n",
    "For more information on how SNNs work, I found [this article](https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942) to be quite helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Initial Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "*For a given pair of lexically similar questions, are the two questions asking for the same information?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Motivation and Business Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Solving the research question would ultimately answer a broader question for someone designing a chatbot:\n",
    "\n",
    "*If a given answer A is appropriate for question Q1, can A also be used to answer question, Q2?*\n",
    "\n",
    "Answering this question is beyond the scope of this project, but it would be the logical next step in terms of the project results would be to apply a (hopefully accurate) ML model to a repository of questions and answers. Rather than generate similar answers for differently worded questions, a chatbot can save a lot of time by first determining whether a new question is similar to another one already in the repository. If so, then the chatbot need only look up the answer to the first question, rather than search for a suitable answer or respond with a disappointing, “I don’t know.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The dataset is sourced from [Quora’s Question Pairs Dataset](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs), which is publicly available. The dataset is large (about 404,300 question pairs) and already processed — there are almost no missing values, and each question pair is neatly classified as being duplicates (1) or not (0). \n",
    "\n",
    "A few favorite pairs:  \n",
    "\n",
    "-457: *What are the life lessons that Batman teaches us?* and *What are the life lessons you can learn from the dark knight?* (duplicates)  \n",
    "-135001: *Why is life so unfair and difficult?* and *Why is life very unfair?* (duplicates)  \n",
    "-214890: *What is the difference between data mining, artificial intelligence and machine learning?* and *What is the difference between data science, artificial intelligence and machine learning?* (close, but not duplicates)  \n",
    "-404055: *Is Donald Trump corrupt?* and *Does Donald Trump think Putin is corrupt?* (not duplicates)\n",
    "\n",
    "Pre-processing of the dataset included removing punctuation, standardizing spelling, lemmatizing words, and embedding words as vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recap of Previous Milestones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-**Milestone 1:** Project proposal in which I identified the dataset to be analyzed as well as my intention to utilize SNNs to train an ML model.  \n",
    "-**Milestone 2:** EDA and preliminary pre-processing using R. The next major decision would be to select a Python module capable of employing SNNs. Eventually, Keras was selected.  \n",
    "-**Milestone 3:** Initial model building and training using Euclidean distance and contrastive loss function. Unfortunately, the initial ML model performed no better than random guessing. The proposed next steps were to revisit the input data and hyperparameters as well as to learn more about Keras.  \n",
    "-**Milestone 4:** Model revision, which fortunately yielded much more promising results after using Manhattan distance and mean squared error as the loss function. After 10 epochs of training the model, the model performed significantly better than random guessing (about 77% for the training set, 76% for the validation set). Soon after this, I was able to train the model for 50 epochs, which led to even better results. This is the model covered in this final iteration of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in the project, we turned to R for performing exploratory data analysis (EDA). This was because R in general, and R Studio in particular, are better suited to performing these types of analyses. The R source code and associated R data objects are available in the `/src` folder.\n",
    "\n",
    "Because I ran into some initial problems in training the model, I decided to start the entire data pre-processing from scratch in Python while keeping the plots generated in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two missing values in the `question_2` column. What are they?  \n",
    "\n",
    "![](./images/missing_vals.png)\n",
    "\n",
    "With only two missing values out of over 400 thousand, it's safe to simply toss these records from the dataset. This is done later in Cell 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values above were interesting because they essentially are asking the same question. So, out of all questions in each column, how many are unique?  \n",
    "\n",
    "<img src=\"./images/distinct_vals.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "Both of these numbers are well below 400,000, or the total number of records. About a third of questions in each column are \"recycled,\" perhaps paired with different versions of similar questions that might be tricky to detect as duplicates (or not).\n",
    "\n",
    "Let's now take a look at how many distinct questions there are when grouping duplicate and non-duplicate questions.  \n",
    "\n",
    "<img src=\"./images/distinct_vals_by_duplicates.png\" style=\"width: 300px;\"/>  \n",
    "\n",
    "There are total of 404,288 questions (about 255,000 tagged as non-duplicates and the rest tagged as duplicates).  \n",
    "\n",
    "Interestingly, there's a greater proportion or distinct questions in non-duplicates compared to duplicates. Perhaps this is because there are more similar-sounding questions that actually aren't duplicates, and an ML model would need more non-similar examples for a given question in order to make an accurate classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Classification Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on the results above, we can see how the dataset is distributed in terms of duplicate and non-duplicate questions. Ideally, the dataset would feature a 50/50 split of duplicates/non-duplicates. But if this isn't the case, then we can do some stratified sampling when we train the model to ensure that both classes are adequately represented.  \n",
    "\n",
    "<img src=\"./images/duplicate_v_non_duplicate.png\" style=\"width: 400px;\"/>  \n",
    "\n",
    "With many more non-duplicates than duplicates, it makes sense to use stratified random sampling when generating a train and test set for the ML model (see Cell 12, Line 6 of Final Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Classification Distribution by Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can plot distinct questions grouped by class, by question (`q1` or `q2`).  \n",
    "\n",
    "<img src=\"./images/distinct_questions.png\" style=\"width: 400px;\"/>  \n",
    "\n",
    "What we can conclude here is that neither class has significantly more distinct questions than the other. In other words, it is not the case  that one question is being repeated much more compared to others (on average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As ever, we'll start by loading the required Python modules for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/michklein/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michklein/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/michklein/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/michklein/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "import contractions\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers import Embedding, Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D, LSTM, Lambda, Dropout\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Change working directory to one level up so that we can load the required datasets from sibling directory\n",
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of importing the data as an R object as last time, we will read in the data from scratch and try alternative pre-processing modules from Keras.\n",
    "\n",
    "Columns are casted as integers, strings, or factors, with the class attribute `is_duplicate` being cast as a factor or category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1_raw</th>\n",
       "      <th>question2_raw</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140497</th>\n",
       "      <td>223256</td>\n",
       "      <td>178264</td>\n",
       "      <td>How would the US respond if China or Russia to...</td>\n",
       "      <td>You are one of the great powers and you have j...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268029</th>\n",
       "      <td>385504</td>\n",
       "      <td>385505</td>\n",
       "      <td>How can I get 10 lpa plus package as a fresher...</td>\n",
       "      <td>How can an M.Sc. in Statistics fresher in Indi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120435</th>\n",
       "      <td>195341</td>\n",
       "      <td>186930</td>\n",
       "      <td>How can I get Meth out of my system in less th...</td>\n",
       "      <td>How do I pass a drug test for meth in 40 hours?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365220</th>\n",
       "      <td>50270</td>\n",
       "      <td>22099</td>\n",
       "      <td>How does Demonetisation of 1000 and 500 rupees...</td>\n",
       "      <td>What will be the impact on real estate by bann...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66259</th>\n",
       "      <td>114870</td>\n",
       "      <td>114871</td>\n",
       "      <td>What does it mean to have a Whiggish view of h...</td>\n",
       "      <td>What does it mean to teach history? What does ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid1    qid2                                      question1_raw  \\\n",
       "id                                                                          \n",
       "140497  223256  178264  How would the US respond if China or Russia to...   \n",
       "268029  385504  385505  How can I get 10 lpa plus package as a fresher...   \n",
       "120435  195341  186930  How can I get Meth out of my system in less th...   \n",
       "365220   50270   22099  How does Demonetisation of 1000 and 500 rupees...   \n",
       "66259   114870  114871  What does it mean to have a Whiggish view of h...   \n",
       "\n",
       "                                            question2_raw is_duplicate  \n",
       "id                                                                      \n",
       "140497  You are one of the great powers and you have j...            0  \n",
       "268029  How can an M.Sc. in Statistics fresher in Indi...            0  \n",
       "120435    How do I pass a drug test for meth in 40 hours?            1  \n",
       "365220  What will be the impact on real estate by bann...            1  \n",
       "66259   What does it mean to teach history? What does ...            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'data/quora_duplicate_questions.tsv'\n",
    "\n",
    "col_type = {'id': 'int',\n",
    "            'qid1': 'int',\n",
    "            'qid2': 'int',\n",
    "            'question1_raw': 'str',\n",
    "            'question2_raw':  'str',\n",
    "            'is_duplicate': 'category'}\n",
    "\n",
    "data = pd.read_csv(filepath, skiprows=[0], names=col_type.keys(), dtype=col_type, delimiter='\\t', index_col='id')\n",
    "\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to check the dataset for missing values and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Records (Before Dropping NAs):\t404290\n",
      "Total Number of Records (After Dropping NAs):\t404287\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Number of Records (Before Dropping NAs):\\t{len(data)}')\n",
    "\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "print(f'Total Number of Records (After Dropping NAs):\\t{len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll start pre-processing the data by converting text to lowercase, removing numerals, and expanding any contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1_raw</th>\n",
       "      <th>question2_raw</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18802</th>\n",
       "      <td>35592</td>\n",
       "      <td>35593</td>\n",
       "      <td>should a person join the core or software indu...</td>\n",
       "      <td>should i join hardware or software industry? i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261600</th>\n",
       "      <td>377779</td>\n",
       "      <td>377780</td>\n",
       "      <td>is it appropriate for a (christian) spouse to ...</td>\n",
       "      <td>my husband is having an emotional affair with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376384</th>\n",
       "      <td>227955</td>\n",
       "      <td>151358</td>\n",
       "      <td>where can i find fairy themed cupcakes in gold...</td>\n",
       "      <td>where can i get free cupcake delivery in gold ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323007</th>\n",
       "      <td>107409</td>\n",
       "      <td>247763</td>\n",
       "      <td>what is the simple meaning of \"once in a blue ...</td>\n",
       "      <td>what is the meaning of \"once in a blue moon \"?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296143</th>\n",
       "      <td>389457</td>\n",
       "      <td>418300</td>\n",
       "      <td>how often do guys regret rejecting a girl?</td>\n",
       "      <td>do girls enjoy rejecting guys?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid1    qid2                                      question1_raw  \\\n",
       "id                                                                          \n",
       "18802    35592   35593  should a person join the core or software indu...   \n",
       "261600  377779  377780  is it appropriate for a (christian) spouse to ...   \n",
       "376384  227955  151358  where can i find fairy themed cupcakes in gold...   \n",
       "323007  107409  247763  what is the simple meaning of \"once in a blue ...   \n",
       "296143  389457  418300         how often do guys regret rejecting a girl?   \n",
       "\n",
       "                                            question2_raw is_duplicate  \n",
       "id                                                                      \n",
       "18802   should i join hardware or software industry? i...            0  \n",
       "261600  my husband is having an emotional affair with ...            0  \n",
       "376384  where can i get free cupcake delivery in gold ...            0  \n",
       "323007     what is the meaning of \"once in a blue moon \"?            1  \n",
       "296143                     do girls enjoy rejecting guys?            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = ['question1_raw', 'question2_raw']\n",
    "\n",
    "for col in questions:\n",
    "    data[col] = data[col].str.lower()\n",
    "    data[col] = data.apply(lambda row: re.sub(r'[0-9]+', '', row[col]), axis=1)\n",
    "    data[col] = data.apply(lambda row: contractions.fix(row[col]), axis=1)\n",
    "    \n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to tokenize queries, which is breaking down queries and sentences into individual words. This will make further pre-processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1_raw</th>\n",
       "      <th>question2_raw</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42597</th>\n",
       "      <td>76736</td>\n",
       "      <td>76737</td>\n",
       "      <td>why does kungfu panda always eat momo?</td>\n",
       "      <td>should i open a momo stall near cheryl and ber...</td>\n",
       "      <td>0</td>\n",
       "      <td>[why, does, kungfu, panda, always, eat, momo, ?]</td>\n",
       "      <td>[should, i, open, a, momo, stall, near, cheryl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129626</th>\n",
       "      <td>208188</td>\n",
       "      <td>208189</td>\n",
       "      <td>what is the evolution of god?</td>\n",
       "      <td>what is an evolute?</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, is, the, evolution, of, god, ?]</td>\n",
       "      <td>[what, is, an, evolute, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12331</th>\n",
       "      <td>23768</td>\n",
       "      <td>23769</td>\n",
       "      <td>what is it like to be a mentor to high school ...</td>\n",
       "      <td>what are the best ways to find mentor for a hi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, is, it, like, to, be, a, mentor, to, hi...</td>\n",
       "      <td>[what, are, the, best, ways, to, find, mentor,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115134</th>\n",
       "      <td>187811</td>\n",
       "      <td>29483</td>\n",
       "      <td>what would you wish for quora?</td>\n",
       "      <td>what is a wish?</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, would, you, wish, for, quora, ?]</td>\n",
       "      <td>[what, is, a, wish, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88813</th>\n",
       "      <td>149319</td>\n",
       "      <td>149320</td>\n",
       "      <td>renewing indian passport while working in diff...</td>\n",
       "      <td>are we overusing gadgets in day-to-day life?</td>\n",
       "      <td>0</td>\n",
       "      <td>[renewing, indian, passport, while, working, i...</td>\n",
       "      <td>[are, we, overusing, gadgets, in, day-to-day, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid1    qid2                                      question1_raw  \\\n",
       "id                                                                          \n",
       "42597    76736   76737             why does kungfu panda always eat momo?   \n",
       "129626  208188  208189                      what is the evolution of god?   \n",
       "12331    23768   23769  what is it like to be a mentor to high school ...   \n",
       "115134  187811   29483                     what would you wish for quora?   \n",
       "88813   149319  149320  renewing indian passport while working in diff...   \n",
       "\n",
       "                                            question2_raw is_duplicate  \\\n",
       "id                                                                       \n",
       "42597   should i open a momo stall near cheryl and ber...            0   \n",
       "129626                                what is an evolute?            0   \n",
       "12331   what are the best ways to find mentor for a hi...            0   \n",
       "115134                                    what is a wish?            0   \n",
       "88813        are we overusing gadgets in day-to-day life?            0   \n",
       "\n",
       "                                                question1  \\\n",
       "id                                                          \n",
       "42597    [why, does, kungfu, panda, always, eat, momo, ?]   \n",
       "129626             [what, is, the, evolution, of, god, ?]   \n",
       "12331   [what, is, it, like, to, be, a, mentor, to, hi...   \n",
       "115134            [what, would, you, wish, for, quora, ?]   \n",
       "88813   [renewing, indian, passport, while, working, i...   \n",
       "\n",
       "                                                question2  \n",
       "id                                                         \n",
       "42597   [should, i, open, a, momo, stall, near, cheryl...  \n",
       "129626                         [what, is, an, evolute, ?]  \n",
       "12331   [what, are, the, best, ways, to, find, mentor,...  \n",
       "115134                             [what, is, a, wish, ?]  \n",
       "88813   [are, we, overusing, gadgets, in, day-to-day, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question1'] = data.apply(lambda row: word_tokenize(row['question1_raw']), axis=1)\n",
    "data['question2'] = data.apply(lambda row: word_tokenize(row['question2_raw']), axis=1)\n",
    "\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are common words (the, a/an, etc.) that won't add much meaning to a query. We can remove them as well as any punctuation marks so that the model is only analyzing words. A simple function can do both steps at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_punc(tokenized_list):\n",
    "    clean_tokens = [word for word in tokenized_list if word not in stopwords and word not in punctuation]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the function to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164769</th>\n",
       "      <td>[used]</td>\n",
       "      <td>[apply, silicone, caulk, without, using, gun]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359842</th>\n",
       "      <td>[save, hair, getting, thin, frizzy]</td>\n",
       "      <td>[stop, hair, thinning]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372288</th>\n",
       "      <td>[stalkbook, app, work]</td>\n",
       "      <td>[summary, app, work]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12608</th>\n",
       "      <td>[navy, reserves, boot, camp, like]</td>\n",
       "      <td>[navy, boot, camp, stories]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260316</th>\n",
       "      <td>[best, retirement, investment, strategies]</td>\n",
       "      <td>[presently, shall, retiring, years, maintain, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         question1  \\\n",
       "id                                                   \n",
       "164769                                      [used]   \n",
       "359842         [save, hair, getting, thin, frizzy]   \n",
       "372288                      [stalkbook, app, work]   \n",
       "12608           [navy, reserves, boot, camp, like]   \n",
       "260316  [best, retirement, investment, strategies]   \n",
       "\n",
       "                                                question2 is_duplicate  \n",
       "id                                                                      \n",
       "164769      [apply, silicone, caulk, without, using, gun]            0  \n",
       "359842                             [stop, hair, thinning]            0  \n",
       "372288                               [summary, app, work]            0  \n",
       "12608                         [navy, boot, camp, stories]            0  \n",
       "260316  [presently, shall, retiring, years, maintain, ...            0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_cols = ['question1', 'question2']\n",
    "\n",
    "for col in cleaned_cols:\n",
    "    data[col] = data.apply(lambda row: remove_stopwords_punc(row[col]), axis=1)\n",
    "    \n",
    "data.loc[:,['question1', 'question2', 'is_duplicate']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far!  \n",
    "\n",
    "The next step is to standardize spelling in the dataset questions, which utilize both American and British spelling conventions. By default, this project will take all British spelling variants and convert them to their American equivalents. However, there is functionality to perform the opposite conversion, since neither spelling convention is inherently \"more correct\" than the other\n",
    "\n",
    "The dictionary of American vs British spelling conventions is sourced from [this site](http://www.tysto.com/uk-us-spelling-list.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dict = 'data/british_american_spelling_dict.csv'\n",
    "\n",
    "with open(filepath_dict) as file:\n",
    "    reader = csv.reader(file)\n",
    "    british_american_dict = dict((row[0], row[1]) for row in reader)\n",
    "    \n",
    "def standard_spellcheck(word_list):\n",
    "    output = []\n",
    "    for word in word_list:\n",
    "        if word in british_american_dict.keys():\n",
    "            output.append(british_american_dict.get(word))\n",
    "        else:\n",
    "            output.append(word)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the function to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1_raw</th>\n",
       "      <th>question1</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>what is your favourite anime character and why?</td>\n",
       "      <td>[favorite, anime, character]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>did ancient people perceive less colours than us?</td>\n",
       "      <td>[ancient, people, perceive, less, colors, us]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question1_raw  \\\n",
       "id                                                        \n",
       "555     what is your favourite anime character and why?   \n",
       "2735  did ancient people perceive less colours than us?   \n",
       "\n",
       "                                          question1 is_duplicate  \n",
       "id                                                                \n",
       "555                    [favorite, anime, character]            1  \n",
       "2735  [ancient, people, perceive, less, colors, us]            0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in cleaned_cols:\n",
    "    data[col] = data.apply(lambda row: standard_spellcheck(row[col]), axis=1)\n",
    "\n",
    "data.loc[[555, 2735],['question1_raw', 'question1', 'is_duplicate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to lemmatize all words. Lemmatization reduces words to their basic grammatical roots. This will make it easier to test that words like \"eat\" and \"eating\" are similar even if they're evaluated as different character strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_list(word_list):\n",
    "    output = []\n",
    "    for word in word_list:\n",
    "        output.append(lemmatizer.lemmatize(word))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the lemmatization function to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68470</th>\n",
       "      <td>pro con transatlantic trade investment partner...</td>\n",
       "      <td>advantage disadvantage transatlantic trade inv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148718</th>\n",
       "      <td>improve writing skill</td>\n",
       "      <td>improve english writing speaking skill</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43762</th>\n",
       "      <td>benefit ban rupee note</td>\n",
       "      <td>consequence rupee note banning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91027</th>\n",
       "      <td>lose weight</td>\n",
       "      <td>lose kilo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175607</th>\n",
       "      <td>historical background cinese worker australian...</td>\n",
       "      <td>historical background cinese worker australian...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "id                                                          \n",
       "68470   pro con transatlantic trade investment partner...   \n",
       "148718                              improve writing skill   \n",
       "43762                              benefit ban rupee note   \n",
       "91027                                         lose weight   \n",
       "175607  historical background cinese worker australian...   \n",
       "\n",
       "                                                question2 is_duplicate  \n",
       "id                                                                      \n",
       "68470   advantage disadvantage transatlantic trade inv...            1  \n",
       "148718             improve english writing speaking skill            1  \n",
       "43762                      consequence rupee note banning            1  \n",
       "91027                                           lose kilo            1  \n",
       "175607  historical background cinese worker australian...            0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in cleaned_cols:\n",
    "    data[col] = data.apply(lambda row: lemmatize_list(row[col]), axis=1)\n",
    "    data[col] = data.apply(lambda row: ' '.join(row[col]), axis=1)\n",
    "    \n",
    "data.loc[:,['question1', 'question2', 'is_duplicate']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating Training and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll take the dataset and split it into training and test sets. We'll start with 80% training and 20% test, though we may adjust this later. The `train_test_split` function in Scikit can automatically stratify the sample so that the proportions in each set represent the proportions in the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['question1', 'question2']], \n",
    "                                                    data['is_duplicate'],\n",
    "                                                    test_size=test_size,\n",
    "                                                    stratify=data['is_duplicate'],\n",
    "                                                    random_state=611)\n",
    "\n",
    "# Cast labels as Numpy arrays and reshape for passing to model later\n",
    "y_train = np.asarray(y_train).astype('float32').reshape(-1,1)\n",
    "y_test = np.asarray(y_test).astype('float32').reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the final set sizes look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set (X):\t323,429 records\n",
      "Test Set (X):\t\t80,858 records\n",
      "\n",
      "Training Set (y):\t323,429 records\n",
      "Test Set (y):\t\t80,858 records\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Set (X):\\t{X_train.shape[0]:,} records')\n",
    "print(f'Test Set (X):\\t\\t{X_test.shape[0]:,} records')\n",
    "print(f'\\nTraining Set (y):\\t{y_train.shape[0]:,} records')\n",
    "print(f'Test Set (y):\\t\\t{y_test.shape[0]:,} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good! We're ready for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing, Encoding, and Padding Textual Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenize, encode, and pad the textual information.\n",
    "\n",
    "**Tokenizing** will involve separating words in a given corpus into discrete units. Typically, each word (separated by spaces) is a single token. Keras provides us with a `Tokenizer` class to help with this.  \n",
    "\n",
    "**Encoding** will involve converting words and phrases into vectors. This is because, like all neural networks, the SNN will recognize only numeric arrays as inputs and not characters.\n",
    "\n",
    "**Padding** will involved resizing word vector arrays so that they are all the same shape for input into the SNN model.\n",
    "\n",
    "One approach to encoding is to assign new values as new words appear. For example, the sentence, \"Give the ball to the man\" would be encoded as `[1, 2, 3, 4, 2, 5]` (note that the word \"the\" is repeated, and so is the index.\n",
    "\n",
    "Another approach is to perform a frequency analysis and assign values based on frequency. In the example above, the word \"the\" would be assigned a value of `1` because it appears the most frequently. This is the approach we'll take.\n",
    "\n",
    "Finally, note that the index starts at `1` rather than `0` as usual. The reason for this will be explained shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Creating a Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create our corpus, that is, the body of text from which indices are organized and assigned. In this case, the corpus will comprise the entire set of questions, all aggregated as one body of text. Reading this corpus won't make much sense, but the purpose here is only to arrange words by frequency and assign values accordingly.\n",
    "\n",
    "The following function can take a dataframe of strings and output a list of combined words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    return df.agg(' '.join, axis=1)\n",
    "\n",
    "corpus = create_corpus(X_train[['question1', 'question2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Encoding the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can instantiate a Keras `Tokenizer` class and encode the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Corpus Word Count:\t3,553,559\n",
      "Distinct Words in Corpus:\t68,967\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(corpus.values)\n",
    "\n",
    "print(f'Total Corpus Word Count:\\t{sum(t.word_counts.values()):,}')\n",
    "print(f'Distinct Words in Corpus:\\t{len(t.word_index) + 1:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our corpus is about 3.6 million words that comprise a little under 70,000 distinct words.\n",
    "\n",
    "Next, we'll map text in our training and test sets to the newly assigned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_q1 = t.texts_to_sequences(X_train['question1'].values)\n",
    "X_train_q2 = t.texts_to_sequences(X_train['question2'].values)\n",
    "\n",
    "X_test_q1 = t.texts_to_sequences(X_test['question1'].values)\n",
    "X_test_q2 = t.texts_to_sequences(X_test['question2'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of what the mapped data looks like compared to the original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Original</th>\n",
       "      <th>Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>279136</td>\n",
       "      <td>acceleration measured unit distance/time^</td>\n",
       "      <td>[2777, 2872, 1431, 558, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321162</td>\n",
       "      <td>gravity look like</td>\n",
       "      <td>[1027, 170, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43014</td>\n",
       "      <td>new rupee gps</td>\n",
       "      <td>[24, 137, 1391]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>383717</td>\n",
       "      <td>evidence support ancient alien theory</td>\n",
       "      <td>[904, 240, 1052, 905, 389]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312111</td>\n",
       "      <td>mean ex-crush stare try get attention</td>\n",
       "      <td>[47, 609, 758, 3324, 700, 2, 2444]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>266555</td>\n",
       "      <td>people still believe communism viable economic...</td>\n",
       "      <td>[5, 102, 247, 3643, 5208, 1173, 103]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>393886</td>\n",
       "      <td>girl played troll doll back 's toy boy play era</td>\n",
       "      <td>[46, 1672, 5106, 6972, 133, 9, 3834, 420, 206,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>187733</td>\n",
       "      <td>blanket statement</td>\n",
       "      <td>[8043, 1459]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>77386</td>\n",
       "      <td>example normative economic statement</td>\n",
       "      <td>[94, 15932, 1173, 1459]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34598</td>\n",
       "      <td>law arrest woman</td>\n",
       "      <td>[224, 4350, 62]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                           Original  \\\n",
       "0  279136          acceleration measured unit distance/time^   \n",
       "1  321162                                  gravity look like   \n",
       "2   43014                                      new rupee gps   \n",
       "3  383717              evidence support ancient alien theory   \n",
       "4  312111              mean ex-crush stare try get attention   \n",
       "5  266555  people still believe communism viable economic...   \n",
       "6  393886    girl played troll doll back 's toy boy play era   \n",
       "7  187733                                  blanket statement   \n",
       "8   77386               example normative economic statement   \n",
       "9   34598                                   law arrest woman   \n",
       "\n",
       "                                             Encoded  \n",
       "0                        [2777, 2872, 1431, 558, 15]  \n",
       "1                                     [1027, 170, 4]  \n",
       "2                                    [24, 137, 1391]  \n",
       "3                         [904, 240, 1052, 905, 389]  \n",
       "4                 [47, 609, 758, 3324, 700, 2, 2444]  \n",
       "5               [5, 102, 247, 3643, 5208, 1173, 103]  \n",
       "6  [46, 1672, 5106, 6972, 133, 9, 3834, 420, 206,...  \n",
       "7                                       [8043, 1459]  \n",
       "8                            [94, 15932, 1173, 1459]  \n",
       "9                                    [224, 4350, 62]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data={'Original': X_train['question1'],\n",
    "                   'Encoded': X_train_q1}).reset_index().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Row 1, note that the word `'like'` is encoded as `4`, meaning that it is the fourth most frequently occurring word in the dataset. In Row 4, you can see that the word `'get'` is the second most frequent word since it's encoded as `2`. This isn't surprising, as these words are often used in search queries. In other words, everything is still looking good, and we can move to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding Encoded Word Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our people-friendly words have been transformed into Keras-friendly numbers, we face another issue. Different-length phrases result in arrays of differing dimensions. This won't do for our SNN model, so the solution is to make every encoded array the same size. Shorter arrays will be populated with the index `0` where no token values exist. This is the reason for reserving the `0` index and starting with `1` above.\n",
    "\n",
    "First, let's look for the maximum length of a given array, that is, the phrase with the maximum number of words in the dataset. This will serve as a starting point for how much to pad the data.\n",
    "\n",
    "We can start by defining a function to find the maximum, median, and 95/95 percentile lengths of any given array in the entire dataset (test and training, Q1 and Q2). We'll also output these summary statistics as a printout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_set_stats(train_1, train_2, test_1, test_2, output='summary'):\n",
    "    \n",
    "    vals = [*train_1, *train_2, *test_1, *test_2]\n",
    "    \n",
    "    total_max = int(max(len(x) for x in vals))\n",
    "    total_med = int(stats.median(len(x) for x in vals))\n",
    "    total_95 = int(np.percentile([len(x) for x in vals], 95))\n",
    "    total_99 = int(np.percentile([len(x) for x in vals], 99))\n",
    "     \n",
    "    if output == 'summary':\n",
    "        print(f'SUMMARY STATISTICS')\n",
    "        print(f'-'*20)\n",
    "        print(f'Max Length:\\t{total_max}')\n",
    "        print(f'Median Length:\\t{total_med}')\n",
    "        print(f'Length 95%ile:\\t{total_95}')\n",
    "        print(f'Length 99%ile:\\t{total_99}')\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    elif output == 'max':\n",
    "        return total_max\n",
    "    \n",
    "    elif output == 'med':\n",
    "        return total_med\n",
    "    \n",
    "    elif output == '95':\n",
    "        return total_95\n",
    "    \n",
    "    elif output == '99':\n",
    "        return total_99\n",
    "    \n",
    "    else:\n",
    "        print('Invalid output argument')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY STATISTICS\n",
      "--------------------\n",
      "Max Length:\t99\n",
      "Median Length:\t5\n",
      "Length 95%ile:\t12\n",
      "Length 99%ile:\t16\n"
     ]
    }
   ],
   "source": [
    "find_set_stats(X_train_q1, X_train_q2, X_test_q1, X_test_q2, output='summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this shows is that the maximum length of a query in the entire dataset is 99 words long. However, this is clearly an outlier. In fact, 99% of the queries in the dataset are 16 words or fewer.\n",
    "\n",
    "If we went with the max value of 99, then every array would be 99 elements long, and most of those elements would be zeroes. In other worlds, we would significantly increase the complexity and processing time of the model without gaining much information. Therefore, we will use 16 as the maximum length for our padding. Queries that are longer will be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_padding = find_set_stats(X_train_q1, X_train_q2, X_test_q1, X_test_q2, output='99')\n",
    "\n",
    "X_train_q1 = pad_sequences(X_train_q1, maxlen=max_len_padding, padding='post')\n",
    "X_train_q2 = pad_sequences(X_train_q2, maxlen=max_len_padding, padding='post')\n",
    "\n",
    "X_test_q1 = pad_sequences(X_test_q1, maxlen=max_len_padding, padding='post')\n",
    "X_test_q2 = pad_sequences(X_test_q2, maxlen=max_len_padding, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the padded data looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Original</th>\n",
       "      <th>Encoded and Padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>279136</td>\n",
       "      <td>acceleration measured unit distance/time^</td>\n",
       "      <td>[2777, 2872, 1431, 558, 15, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321162</td>\n",
       "      <td>gravity look like</td>\n",
       "      <td>[1027, 170, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43014</td>\n",
       "      <td>new rupee gps</td>\n",
       "      <td>[24, 137, 1391, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>383717</td>\n",
       "      <td>evidence support ancient alien theory</td>\n",
       "      <td>[904, 240, 1052, 905, 389, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312111</td>\n",
       "      <td>mean ex-crush stare try get attention</td>\n",
       "      <td>[47, 609, 758, 3324, 700, 2, 2444, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>266555</td>\n",
       "      <td>people still believe communism viable economic...</td>\n",
       "      <td>[5, 102, 247, 3643, 5208, 1173, 103, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>393886</td>\n",
       "      <td>girl played troll doll back 's toy boy play era</td>\n",
       "      <td>[46, 1672, 5106, 6972, 133, 9, 3834, 420, 206,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>187733</td>\n",
       "      <td>blanket statement</td>\n",
       "      <td>[8043, 1459, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>77386</td>\n",
       "      <td>example normative economic statement</td>\n",
       "      <td>[94, 15932, 1173, 1459, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34598</td>\n",
       "      <td>law arrest woman</td>\n",
       "      <td>[224, 4350, 62, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                           Original  \\\n",
       "0  279136          acceleration measured unit distance/time^   \n",
       "1  321162                                  gravity look like   \n",
       "2   43014                                      new rupee gps   \n",
       "3  383717              evidence support ancient alien theory   \n",
       "4  312111              mean ex-crush stare try get attention   \n",
       "5  266555  people still believe communism viable economic...   \n",
       "6  393886    girl played troll doll back 's toy boy play era   \n",
       "7  187733                                  blanket statement   \n",
       "8   77386               example normative economic statement   \n",
       "9   34598                                   law arrest woman   \n",
       "\n",
       "                                  Encoded and Padded  \n",
       "0  [2777, 2872, 1431, 558, 15, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [1027, 170, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "2  [24, 137, 1391, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [904, 240, 1052, 905, 389, 0, 0, 0, 0, 0, 0, 0...  \n",
       "4  [47, 609, 758, 3324, 700, 2, 2444, 0, 0, 0, 0,...  \n",
       "5  [5, 102, 247, 3643, 5208, 1173, 103, 0, 0, 0, ...  \n",
       "6  [46, 1672, 5106, 6972, 133, 9, 3834, 420, 206,...  \n",
       "7  [8043, 1459, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8  [94, 15932, 1173, 1459, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "9  [224, 4350, 62, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data={'Original': X_train['question1'],\n",
    "                   'Encoded and Padded': X_train_q1.tolist()}).reset_index().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the initial padded data values are identical to the encoded data from before, only that the array is populated with the reserve index `0` to fill the array if the phrase is less than 16 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Data to Space with Global Vectors (GloVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now mapped our textual queries to integer, but that begs the question: how do we know how similar words are to each other? After all, if we compare some of the top words `like` and `get`, it's clear that these words aren't semantically related to each other at all, even if they're close together in their integer values.\n",
    "\n",
    "The solution is to use the Global Vectors (GloVe) model, which is an open-source algorithm for representing words as vectors in *n*-dimensional space. Vectors for synonymous words like *little* and *small* are close to each other, as are words that share the same context, such as *kitchen* and *eat*.\n",
    "\n",
    "By \"close,\" we mean that vectors have a small Euclidean distance, which will be the standard used to calculate word similarities in this model.\n",
    "\n",
    "The theory behind the algorithm is detailed in [this paper by Pennington, Socher, and Manning (2014)](https://nlp.stanford.edu/pubs/glove.pdf). In addition to developing the algorithm, the authors produced a repository of 400,000 words sourced from Wikipedia articles. Each word is described by an *n*-dimensional vector, with 300 being the standard size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe embeddings for 400,000 words are publicly available for download. We'll use the standard 300-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400,000 word vectors of 300 dimensions each.\n"
     ]
    }
   ],
   "source": [
    "glove_words = {}\n",
    "\n",
    "filepath = 'data/glove.6B.300d.txt'\n",
    "\n",
    "with open(filepath) as file:\n",
    "    for line in file:\n",
    "        vals = line.split()\n",
    "        word = vals[0]\n",
    "        coords = np.asarray(vals[1:], dtype='float32')\n",
    "        \n",
    "        glove_words[word] = coords\n",
    "\n",
    "glove_len = len(glove_words)    \n",
    "glove_dim = len(glove_words.get(\"can\")) \n",
    "\n",
    "print(f'Loaded {glove_len:,} word vectors of {glove_dim} dimensions each.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just what we expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the Corpus to GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an array of GloVe embeddings for each word in our corpus. With about 69,000 words in the corpus, chances are that the most common words will appear in the list of 400,000 GloVe embeddings. However, less-frequent words may not appear on the list at all. These words will simply be populated with zeroes in the array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_glove = []\n",
    "corpus_size = len(t.word_index) + 1\n",
    "\n",
    "corpus_array = np.empty((corpus_size, glove_dim))\n",
    "\n",
    "glove_vec = None\n",
    "\n",
    "\n",
    "for word, idx in t.word_index.items():\n",
    "#     If a word in the corpus appears in the GloVe embeddings, get the coordinates for that word vector. Else, add the word to the list of words not found.\n",
    "    if word in glove_words.keys():\n",
    "        glove_vec = glove_words.get(word)\n",
    "    else:\n",
    "        not_in_glove.append(word)\n",
    "    \n",
    "#     Then, add the vector coordinates to the corpus array if the word was found. Otherwise, populate the array with zeroes for that word\n",
    "    if glove_vec is not None:\n",
    "        corpus_array[idx] = glove_vec\n",
    "    else:\n",
    "        corpus_array[idx] = np.zeros(glove_dim)\n",
    "        \n",
    "    glove_vec = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? First, let's see how many words weren't found in the GloVe embeddings file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20,297\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(not_in_glove):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 20,000 words weren't found, which is a significant number. We'll continue for now, but we may want to revisit this if our model still doesn't perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is essentially ready to be passed into the Siamese model, which leaves with one last task: setting up the Siamese model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Manhattan Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous milestone, we used the Euclidean distance to calculate the similarity of two embedded word vectors (a smaller distance means that the words are more similar to each other).\n",
    "\n",
    "After some more research, it might make more sense to consider Manhattan distance rather than Euclidean distance. This is because it's not quite clear what vector coordinates represent in the embedded vector space. Rather than representing continuous variables, it could very well be that the coordinates reflect black-box categorical attributes that are determined by the GloVe algorithm. If this is the case, then it would make more sence to consider Manhattan distance, which is more appropriate for categorical attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we used contrastive loss as the loss function for the model:\n",
    "\n",
    "$Y \\times D^2 + (1-Y) \\times max(margin - D, 0)^2$\n",
    "\n",
    "where:\n",
    "\n",
    "-$Y$ is the label value (1 or 0)  \n",
    "-$D$ is the calculated Euclidean distance.  \n",
    "-$margin$ is a parameter we set and essentially acts as a radius of acceptable similarity. In our case, two vectors that are colocated within the margin radius will be labeled as duplicates.\n",
    "\n",
    "However, this iteration will try to use a built-in loss function, mean squared error, which is another appropriate function for models that calculate distances. First, the square term means that there won't be any negative distances/values.The square term also means that words that are far apart from each other are penalized much more than embedded word vectors that are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Manhattan Distance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before initializing the model, we'll need to define the functions to calculate Manhattan distance. Since these functions will be called within the model itself, we won't use conventional Python Math and Numpy operations, but rather the set of functions that come with the Keras backend library (abbreviated below as `K`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The manhatten_dist function accepts a tuple of two vectors, which are then unpacked. The Manhattan distance is then calculated.\n",
    "def manhattan_dist(x, y):\n",
    "    return K.exp(-K.sum(K.abs(x-y), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the Model and Building Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize the model! We'll start by building the network and adding layers. However, the approach will be slightly different than what was done in Milestone 3.\n",
    "\n",
    "Instead of setting up Dense and Flatten layers, we'll focus only on the Embedding, LSTM, and Lamba layers.\n",
    "\n",
    "For the first SNN layer, the `Embedding` layer, we'll pass the following arguments:  \n",
    "\n",
    "-The `input_dim` will match the number of words in the dataset (about 82,000)  \n",
    "-The `output_dim` will match the number of dimensions in the GloVe embeddings data (50 in this first iteration)  \n",
    "-`weights` will be taken from the `corpus_array` created earlier  \n",
    "-The `input_length` will correspond the padded vector length (17)  \n",
    "-`trainable` will be set to `False` so that weights are not updated during training  \n",
    "-`name` will be `embedding_layer`\n",
    "\n",
    "One known problem with neural networks that process sequential data (like text) is that they struggle with retaining information from earlier steps to later ones. To mitigate this, we'll add a long short-term memory (LSTM) layer that's shared between the two embedding layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 04:32:45.013202: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "n_hidden=50\n",
    "\n",
    "input_1 = Input(shape=(max_len_padding,),dtype='int32')\n",
    "input_2 = Input(shape=(max_len_padding,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(input_dim=corpus_size,\n",
    "                            output_dim=glove_dim,\n",
    "                            weights=[corpus_array],\n",
    "                            input_length = max_len_padding,\n",
    "                            trainable=False,\n",
    "                            name='embedding_layer')\n",
    "\n",
    "encoded_input_1 = embedding_layer(input_1)\n",
    "encoded_input_2 = embedding_layer(input_2)\n",
    "\n",
    "lstm_shared_layer = LSTM(n_hidden, name='lstm_shared_layer')\n",
    "\n",
    "output_1 = lstm_shared_layer(encoded_input_1)\n",
    "output_2 = lstm_shared_layer(encoded_input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the Lambda layer will calculate the Manhattan distance between the two embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Lambda(lambda x: manhattan_dist(x[0], x[1]), output_shape=lambda x: (x[0][0], 1))([output_1, output_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a Keras Model instance and specify the input (the input vectors) and output (their Manhattan distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNN_model = Model(inputs=[input_1, input_2], outputs=[dist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adadelta(clipnorm=1.25)\n",
    "\n",
    "SNN_model.compile(loss='mean_squared_error', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Siamese model is all set up, so we just need to train it. We'll use 10 epochs for now, through the final milestone utilize more. Given the memory limitations of this machine, it would probably be better to define a batch size of 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 115s 3s/step - loss: 0.2161 - accuracy: 0.6710 - val_loss: 0.2012 - val_accuracy: 0.7030\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 112s 3s/step - loss: 0.1922 - accuracy: 0.7201 - val_loss: 0.1881 - val_accuracy: 0.7284\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 111s 3s/step - loss: 0.1804 - accuracy: 0.7417 - val_loss: 0.1794 - val_accuracy: 0.7443\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 111s 3s/step - loss: 0.1722 - accuracy: 0.7574 - val_loss: 0.1730 - val_accuracy: 0.7543\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 109s 3s/step - loss: 0.1659 - accuracy: 0.7693 - val_loss: 0.1682 - val_accuracy: 0.7641\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 108s 3s/step - loss: 0.1608 - accuracy: 0.7790 - val_loss: 0.1645 - val_accuracy: 0.7713\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 109s 3s/step - loss: 0.1567 - accuracy: 0.7865 - val_loss: 0.1618 - val_accuracy: 0.7765\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1532 - accuracy: 0.7933 - val_loss: 0.1593 - val_accuracy: 0.7805\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 131s 4s/step - loss: 0.1500 - accuracy: 0.7987 - val_loss: 0.1575 - val_accuracy: 0.7842\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 136s 4s/step - loss: 0.1474 - accuracy: 0.8035 - val_loss: 0.1558 - val_accuracy: 0.7883\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 123s 4s/step - loss: 0.1450 - accuracy: 0.8073 - val_loss: 0.1546 - val_accuracy: 0.7892\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 111s 3s/step - loss: 0.1430 - accuracy: 0.8113 - val_loss: 0.1536 - val_accuracy: 0.7915\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1411 - accuracy: 0.8145 - val_loss: 0.1529 - val_accuracy: 0.7924\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 112s 3s/step - loss: 0.1394 - accuracy: 0.8172 - val_loss: 0.1522 - val_accuracy: 0.7947\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 117s 4s/step - loss: 0.1378 - accuracy: 0.8209 - val_loss: 0.1520 - val_accuracy: 0.7947\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 117s 4s/step - loss: 0.1363 - accuracy: 0.8233 - val_loss: 0.1512 - val_accuracy: 0.7947\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 111s 3s/step - loss: 0.1350 - accuracy: 0.8257 - val_loss: 0.1504 - val_accuracy: 0.7985\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 112s 3s/step - loss: 0.1338 - accuracy: 0.8278 - val_loss: 0.1502 - val_accuracy: 0.7967\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 116s 4s/step - loss: 0.1327 - accuracy: 0.8297 - val_loss: 0.1499 - val_accuracy: 0.7979\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 112s 3s/step - loss: 0.1316 - accuracy: 0.8319 - val_loss: 0.1499 - val_accuracy: 0.7989\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 111s 3s/step - loss: 0.1305 - accuracy: 0.8336 - val_loss: 0.1496 - val_accuracy: 0.7976\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 109s 3s/step - loss: 0.1294 - accuracy: 0.8355 - val_loss: 0.1486 - val_accuracy: 0.8008\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1285 - accuracy: 0.8376 - val_loss: 0.1486 - val_accuracy: 0.8006\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 111s 3s/step - loss: 0.1276 - accuracy: 0.8391 - val_loss: 0.1486 - val_accuracy: 0.7993\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 112s 3s/step - loss: 0.1268 - accuracy: 0.8400 - val_loss: 0.1483 - val_accuracy: 0.8012\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 112s 3s/step - loss: 0.1262 - accuracy: 0.8412 - val_loss: 0.1480 - val_accuracy: 0.8016\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1252 - accuracy: 0.8429 - val_loss: 0.1482 - val_accuracy: 0.8007\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 109s 3s/step - loss: 0.1247 - accuracy: 0.8437 - val_loss: 0.1476 - val_accuracy: 0.8007\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1237 - accuracy: 0.8457 - val_loss: 0.1475 - val_accuracy: 0.8009\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 111s 3s/step - loss: 0.1228 - accuracy: 0.8472 - val_loss: 0.1476 - val_accuracy: 0.8020\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 109s 3s/step - loss: 0.1224 - accuracy: 0.8480 - val_loss: 0.1476 - val_accuracy: 0.8017\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 114s 3s/step - loss: 0.1218 - accuracy: 0.8490 - val_loss: 0.1475 - val_accuracy: 0.8015\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 109s 3s/step - loss: 0.1212 - accuracy: 0.8504 - val_loss: 0.1472 - val_accuracy: 0.8021\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1207 - accuracy: 0.8510 - val_loss: 0.1470 - val_accuracy: 0.8018\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1200 - accuracy: 0.8522 - val_loss: 0.1474 - val_accuracy: 0.8017\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1194 - accuracy: 0.8532 - val_loss: 0.1467 - val_accuracy: 0.8031\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 113s 3s/step - loss: 0.1188 - accuracy: 0.8543 - val_loss: 0.1468 - val_accuracy: 0.8040\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 111s 3s/step - loss: 0.1181 - accuracy: 0.8552 - val_loss: 0.1465 - val_accuracy: 0.8034\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 109s 3s/step - loss: 0.1176 - accuracy: 0.8564 - val_loss: 0.1466 - val_accuracy: 0.8042\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 108s 3s/step - loss: 0.1171 - accuracy: 0.8574 - val_loss: 0.1465 - val_accuracy: 0.8036\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 111s 3s/step - loss: 0.1168 - accuracy: 0.8577 - val_loss: 0.1464 - val_accuracy: 0.8038\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 109s 3s/step - loss: 0.1161 - accuracy: 0.8587 - val_loss: 0.1474 - val_accuracy: 0.8030\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 119s 4s/step - loss: 0.1164 - accuracy: 0.8586 - val_loss: 0.1464 - val_accuracy: 0.8042\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1153 - accuracy: 0.8601 - val_loss: 0.1464 - val_accuracy: 0.8042\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 129s 4s/step - loss: 0.1148 - accuracy: 0.8615 - val_loss: 0.1466 - val_accuracy: 0.8045\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 115s 3s/step - loss: 0.1145 - accuracy: 0.8617 - val_loss: 0.1465 - val_accuracy: 0.8039\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1139 - accuracy: 0.8628 - val_loss: 0.1465 - val_accuracy: 0.8045\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1135 - accuracy: 0.8632 - val_loss: 0.1471 - val_accuracy: 0.8033\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 110s 3s/step - loss: 0.1132 - accuracy: 0.8636 - val_loss: 0.1464 - val_accuracy: 0.8040\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 108s 3s/step - loss: 0.1126 - accuracy: 0.8653 - val_loss: 0.1467 - val_accuracy: 0.8041\n"
     ]
    }
   ],
   "source": [
    "fit_model = True\n",
    "\n",
    "if fit_model:\n",
    "    SNN_model.fit([X_train_q1, X_train_q2], y_train,\n",
    "                  epochs=50, batch_size=10000,\n",
    "                  validation_data=([X_test_q1, X_test_q2], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to save the model and weights for quick and easy access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_model = 'results/SNN_model_03.json'\n",
    "filepath_weights = 'results/SNN_model_03_weights.h5'\n",
    "\n",
    "SNN_model_to_json = SNN_model.to_json()\n",
    "\n",
    "with open(filepath_model, 'w') as file:\n",
    "    file.write(SNN_model_to_json)\n",
    "    \n",
    "SNN_model.save_weights(filepath_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did the model perform? Much better than any of the previous iterations. With an accuracy score of about about 87% for the training set and 80% for the validation set, we can confidently say that the model is a success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion (the Point of it All)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model is much more effective than the initial model, and it's clear that using Manhattan distance rather than Euclidean distance significantly improved accuracy. \n",
    "\n",
    "The conclusion we can make is that it is indeed possible to determine whether two queries are semantically similar. With enough training data and the use of a Siamese Neural Network, we can input two question strings and eventually output their similarity (being classed as duplicates or non-duplicates). In short, this model shows promise in having a business value in detecting questions that have been answered before.\n",
    "\n",
    "At the same time, I would be hesitant to keep training the model over more epochs. As can be seen in the results above, the accuracy of the test and validations sets have begun to diverge. This means that there's a risk that the model might be overfitting to the data at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's clear that the model is off to a good start, there's still much to be done! Given more time and Keras knowledge, I would have liked to have done the following:  \n",
    "\n",
    "**1. Improve the model:** As mentioned above, improving the model is more than simply running more epochs. I would be interested in further deepening my understanding of neural networks improve model performance without running into overfitting issues.  \n",
    "\n",
    "**2. Make the model more specific:** One of the problems of NLP tasks is that different words have different meanings, depending on the context. For instance, a \"sequence\" in mathematics is quite different than a \"sequence\" in genomics. In the former case, similar words might include \"geometric,\" \"series,\" and so on. In the latter case, similar words would include \"genome,\" \"DNA,\" \"guanine,\" and so on. The issue with GloVe, or any other vector representation algorithm for that matter, is that one is forced to compromise and develop vector representations that satisfy all contexts equally poorly. Therefore, a truly effective ML model should both contain context-specific data (rather than general website queries) as well as an embedding algorithm that vectorizes words based on specific contexts (whether that's genomics, marine biology, or something else).\n",
    "\n",
    "**3. Employ the Model:** Due to time constraints, I was not able to develop any sort of interactive app that utilizes the model, that is, a way for a user to input two questions and find out if the model classifies them as duplicates or not. Should I be able to achieve the first two recommendations above, developing an interative platform would be the next step.  \n",
    "\n",
    "That said, it's been an exciting semester, and I'm glad to have had the chance to challenge myself in this course while developing a working ML model along the way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_final_project]",
   "language": "python",
   "name": "conda-env-ml_final_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
